1.1 강화학습
- 쉽지만 추상적인 버전(시행착오를 통해 발전해 나가는 과정)
- 어렵지만 좀 더 정확한 버전(순차적 의사결정 문제에서 누적 보상을 최대화 하기 위해 시행착오를 통해 행동을 교정하는 학습 과정)

1.2 순차적 의사결정 문제
- 강화학습이 풀고자 하는 문제(연이은 행동을 잘 선택해야 하는 문제)

1.3 보상
- 보상이란 의사결정을 얼마나 잘하고 있는지 알려주는 신호
- 강화 학습의 목적은 과정에서 받는 보상의 총합, 누적보상을 최대화하는 것
1.3-1 첫번째 특징(어떻게 X, 얼마나 O)
- 보상은 어떻게 해야하는지가 중요한 것이 아니라, 얼마나 잘하고 있는지 평가를 해주는 것이다.
- 보상이 어떻게 해야 할지를 직접적으로 알려주지 않지만, 사후적으로 보상이 낮았던 행동들은 덜 하고, 보상이 높았던 행동들은 더 하면서 보상을 최대화하도록 행동을 조금씩 수정해 나가는 것이다.
1.3-2 두번째 특징(스칼라)
- 스칼라의 특징을 가지고 있다.(크기를 나타내는 값만 있으므로 하나의 목적만을 가져야 한다.)
- 스칼라로 표현하는 방법(가중치), 문제가 복잡할 경우 문제를 단순화하여 하나의 목표를 설정해야 한다.
1.3-3 세번째 특징(희소하고 지연된 보상)
- 보상은 선택했던 행동의 빈도에 비해 훨씬 가끔 주어지거나, 행동이 발생한 후 한참 뒤에 주어질 수도 있다. 이 때문에 행동과 보상의 연결이 어려워진다.
- 보상이 희소할수록 학습이 어려워지고 이런 문제를 해결하기 밸류 네트워크(value network)등의 다양한 아이디어가 등장하였다.

1.4 에이전트와 환경
- 에이전트가 액션을 하고 그에 따라 상황이 변하는 것을 하나의 루프라 했을 때 이 루프가 끊임없이 반복되는 것을 순차적 의사결정 문제라 할 수 있다.
- 에이전트 : 강화학습의 주인공, 학습하는 대상이며 동시에 환경 속에서 행동하는 개체, 어떤 액션을 할지 정하는 것이 주된 역할
	1. 현재 상황 st에서 어떤 액션을 해야 할지 at를 결정
	2. 결정된 행동 at를 환경을 보냄
	3. 환경으로부터 그에 따른 보상과 다음 상태의 정보를 받음
- 환경 : 에이전트를 제외한 모든 요소, 상태 변화를 일으키는 역할을 담당, 행동의 결과를 알려주는 것
	- 상태 : 현재 상태에 대한 모든 정보를 숫자로 표현하여 기록
	1. 에이전트로부터 받은 액션 at를 통해서 상태 변화를 일으킴
	2. 그 결과 상태는 st -> st+1로 바뀜
	3. 에이전트에게 줄 보상 rt+1도 함께 계산
	4. st+1과 rt+1을 에이전트에게 전달
- 에이전트가 st에서 at를 시행하고, 이를 통해 환경 st+1로 바뀌면, 즉 에이전트와 환경이 한 번 상호 작용하면 하나의 루프가 끝납니다.(한 틱이 지났다고 표현)

1.5 강화학습의 위력
- 병렬성의 힘
- 자가 학습의 매력