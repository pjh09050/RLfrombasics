딥 Q러닝

가치 기반 에이전트는 명시적 정책(explicit policy)가 따로 없다. 그렇기 때문에 𝜋가 따로 없다.
𝜋없이 어떻게 액션을 선택할 수 있을까? --> 액션-가치함수 q(s,a)를 이용하는 것이다.

q(s,a)는 각 상태 s에서 액션별 가치를 나타낸다.
따라서 각 상태에서 가장 가치가 높은 액션을 선택하는 식으로 정책을 만들 수 있다.
가치 함수는 밸류만 평가하는 함수인데 이를 마치 정책 함수처럼 사용하는 것이다.
---> 이런 경우의 정책 함수를 내재된 정책(implicit policy)라고 한다.

딥 Q러닝은 q(s,a)를 내재된 정책으로 사용한다.
====================================================================================
이론적 배경 - Q러닝
Q러닝은 벨만 최적방정식을 이용해 최적 액션-밸류인 Q*(s,a)를 학습하는 내용이다.
딥 Q러닝은 Q러닝에 뉴럴넷으로 확장하기만 하면 된다.

테이블이 아닌 뉴럴넷을 이용하여 Q(s,a) 함수로 표현하기 때문에 Q𝜃(s,a)라 표기한다.
𝜃는 뉴럴넷의 파라미터 벡터이다.

손실 함수를 정의할 때에는 기댓값 연산자가 반드시 필요하다.
같은 상태 s에서 같은 액션 a를 선택한다 하더라도 매번 다른 상태에 도달할 수 있기 때문이다.
실제로 뉴럴넷을 업데이트할 때는 샘플 기반 방법론으로 기댓값 연산자를 무시하고 계산할 수 있다.
데이터를 여러 개 모아서 그 평균을 이용해 업데이트한다.
𝜃를 계속해서 업데이트해 나가면 Q𝜃(s,a)는 점점 최적의 액션-가치 함수 Q*(s,a)에 가까워질 것이다.

미니 배치(mini-batch) : 복수의 데이터를 모아 놓은 것
미니 배치를 이용해 업데이트 하는 방식을 미니 배치 업데이트라고 부른다.
샘플 몇 개를 뽑아서 미니 배치를 구성할지는 자유이다.
하나의 미니 배치를 구성하는 데 몇 개의 데이터를 사용할 것인가를 다른 말로 미니 배치의 크기 혹은 미니 배치 사이즈라고 한다.

미니 배치의 크기가 커질수록 더 정확한 그라디언트를 계산할 수 있지만, 
그만큼 한 번에 소모해버리는 데이터가 많아지므로 어느 정도 트레이드 오프가 있다고 할 수 있다.

s'에서 Q𝜃에 대한 greedy를 이용하여 액션 a'을 선택
여기서 선택한 액션은 실제로 실행되지는 않으며, 오로지 업데이트를 위한 계산에만 사용되는 부분이다. ----> off-policy 학습
=======================================================================================
익스피리언스 리플레이와 타깃 네트워크

익스피리언스 리플레이는 "겪었던 경험을 재사용하면 더 좋지 않을까?"이다. -> off-policy의 장점
경험은 여러 개의 에피소드로 이루어져있고, 에피소드는 여러 개의 상태 전이(transition)로 이루어져 있다.
하나의 상태 전이 et는 (st,at,rt,st+1)로 표현할 수 있다.
"상태 st에서 액션 at를 했더니 보상 rt을 받고 다음 상태 st+1에 도착하였다"의 뜻이다.
하나의 상태 전이가 하나의 데이터이다.

리플레이 버퍼
- 버퍼에다가 가장 최근 데이터 n개를 저장해 놓자는 아이디어이다.
- 학습할 때는 이 버퍼에서 임의로 데이터를 뽑아서 사용한다.
장점
- 랜덤하게 뽑다 보면 각각의 데이터는 여러 번 재사용될 수 있다. ---> 데이터의 효율성을 올려준다.
- 다양한 데이터 학습을 하면 한 게임 안에서 발생한 연속된 데이터를 사용할 때 보다 
각각의 데이터 사이 상관성이 작아서 효율적으로 학습할 수 있다. -> 성능 개선에 큰 역할을 함

익스피리언스 리플레이를 사용할 때의 주의점
- off-policy 알고리즘에만 사용 가능
- 리플레이 버퍼에 쌓여 있는 데이터는 현재 학습을 받고 있는 정책이 아니라 그보다 과거의 정책이 생성한 데이터이기 때문이다.
- 해당 데이터를 생성한 행동 정책과 현재 학습을 받고 있는 타깃 정책이 서로 다른 정책이다.
- 과거의 나는 현재의 나와 다르기 때문에 off-policy 상황이다.
- 과거에 내가 쌓은 경험을 가지고 현재의 내가 배우고 있는 셈

별도의 타깃 네트워크(Target Network)
- Q러닝에서는 R+𝛾maxQ𝜃(s',a')이 정답으로 사용되기 때문에 정답이 𝜃에 의존적이다.
- 정답을 계산할 때 사용하는 Q_target 네트워크와 학습을 받고 있는 Q 네트워크
- 뉴럴넷을 학습할 때 정답지가 자주 변하는 것은 학습의 안정성을 매우 떨어뜨린다. -> 별도의 타깃 네트워크 등장
- 그래서 𝜃가 업데이트 될 때마다 정답에 해당하는 값이 계속해서 변하며 이는 안정적인 학습에 해가 된다.

- 뉴럴넷을 학습할 때 정답지가 자주 변하는 것은 학습의 안정성을 매우 떨어뜨린다.
그래서 타깃 네트워크의 아이디어가 나온다. 정답을 계산할 떄 사용되는 타깃 네트워크와 학습을 받고 있는 Q네트워크
그리고 정답지를 계산할 때 사용하는 네트워크의 파라미터를 잠시 얼려두는 것이다. 변하지 않도록 얼린 파라미터를 𝜃i-라고 표기
𝜃i-를 고정해놓고 정답지를 계산하면 정답이 안정적인 분포를 가지게 된다. 그 사이 학습을 받고 있는 네트워크의 파라미터는 업데이트된다.
일정 주기마다 얼려 놓았던 𝜃i-를 최신 파라미터로 교체해준다. 

정리하면 학습 도중에는 똑같이 생긴 두 쌍의 파라미터가 사용된다.
학습 대상이 되는 Q네트워크의 파라미터 𝜃i와 정답지 계산에 쓰이는 파라미터 𝜃i-가 공존한다.
