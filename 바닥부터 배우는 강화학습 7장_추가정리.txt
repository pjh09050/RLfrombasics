Pytorch의 다양한 Optimizer 클래스

1. Stochastic Gradient Descent (SGD) : learning rate와 momentum 조절 (가장 기본적)
2. Adagrad : learning rate를 동적으로 조절하여 학습을 안정화시킴
3. RMSprop : Adagrad의 단점을 보완한 Optimizer, learning rate를 적응적으로 조절
4. Adam : Momentum 방법과 Adagrad 방법을 융합한 방법, 기울기의 이동 평균값과 제곱의 이동 평균값을 모두 고려하여 lr 조절
	momentum과 비슷하게 경사하강법의 속도를 빠르게 하고, lr의 크기를 가변적으로 조절
5. Adamax : Adam Optimizer의 변형으로, learning rate를 적응적으로 조절하면서 기존 Adam Optimizer의 문제점을 보완함

============================================================================================

역전파 알고리즘(backpropagation) - 1~4 반복
- 신경망 모델에서 gradient를 계산하여 모델의 가중치(weight)를 업데이트하는 알고리즘

1. Forward pass 
- 입력 데이터를 모델에 통과시킨다. 
	(이때 모델은 각 layer에서 w와 b를 사용하여 input을 선형 변환하고, 활성화 함수를 적용하여 비선형성을 추가한다.)

2. Loss function
- 모델의 출력값과 정답(target)을 비교하여 loss를 계산한다. (cross-entropy나 MSE 등의 함수로 정의)

* cross-entropy : 두 확률 분포 간의 유사도를 비교하는 데 사용, 모델이 예측한 클래스와 실제 클래스 간의 차이를 계산

3. Backward pass
- loss값을 이용하여 gradient를 계산한다. chain relu를 사용하여 각 layer에서 역방향(backward)으로 전달된다.
- 마지막 layer의 gradient는 출력값과 target의 차이를 구한 뒤, 이전 layer에서 계산된 gradient와 곱하여 전달된다.

4. Optimizer를 사용하여 가중치 업데이트
- optimizer은 gradient를 사용하여 가중치를 업데이트한다.
- optimizer은 learning rate 등의 하이퍼파라미터를 사용하여 가중치 업데이트의 크기를 조절한다.

============================================================================================

뉴렐넷에서 사용되는 비선형 함수 종류
1. 시그모이드 함수 (Sigmoid function)
- 입력값을 0~1 사이의 값으로 변환하는 함수, 이진 분류에서 사용된다.

2. 하이퍼볼릭 탄젠트 함수 (Hyperbolic tangent function)
- 입력값을 -1~1 사이의 값으로 출력된다.

3. ReLU (Rectified Linear Unit)
- max(0,x)

4. Leaky ReLU
- ReLU 변형, 입력값이 음수일 경우 기울기를 작은 값으로 유지하여 뉴런이 '죽는 문제'를 완화
- max(0.1x, x)

5. 소프트맥스 함수 (Softmax function)
- 다중 클래스 분류 문제에서 사용, 입력값을 클래스별 확률값으로 변환

===========================================================================================

