가치 기반 에이전트

* 모델 프리 상황, 상태 공간과 액션 공간이 매우 커서 밸류를 일일이 테이블에 담지 못하는 상황

강화학습에 뉴럴넷을 접목시키는 접근법은 크게 2가지가 있다.
1. 가치 함수 𝑣_𝜋(s)나 q_𝜋(s,a)를 뉴럴넷으로 표현하는 방식
2. 정책 함수 𝜋(𝑎│𝑠) 자체를 뉴럴넷으로 표현하는 방식

에이전트의 분류 (가치 기반, 정책 기반, 액터 크리틱)

가치 기반 에이전트
- 가치 함수에 근거하여 액션을 선택, q(s,a)의 값을 보고 액션을 선택
- 상태 s에서 선택할 수 있는 액션들 중에서 가장 밸류가 높은 액션을 선택하는 방식
- SARSA와 Q러닝으로 학습했던 에이전트가 가치 기반 에이전트에 속한다.
- 가치 기반 에이전트가 액션을 선택할 때 가치 함수만 있으면 되기 때문에 정책 함수가 따로 없다.
- q(s,a)가 곧 정책 함수 역할을 하는 것이다.

정책 기반 에이전트(9장) ( 𝜋(𝑎│𝑠) )
- 정책 함수 𝜋(𝑎│𝑠)를 보고 직접 액션을 선택 (가치 함수는 쓰이지 않는다.)
- 밸류를 보고 액션을 선택하지 않으며, 가치 함수를 따로 두지 않는다. (가치 기반 에이전트와의 차이점)
- 𝜋만 있으면 에이전트는 MDP안에서 경험을 쌓을 수 있고, 경험을 이용해 학습 과정에서 𝜋를 강화한다. (정책을 강화)

액터-크리틱(9장)
- 가치 함수와 정책 함수 모두 사용
- 액터는 "행동하는 녀석", 정책 𝜋를 뜻함 
- 크리틱은 "비평가", 가치 함수 v(s)또는 q(s,a)를 가리킨다.
- 행동하는 𝜋와 평가하는 v(혹은 q)가 함께 존재한다.

뉴럴넷을 이용하여 가치 기반 에이전트를 학습
- 정책 𝜋가 고정되어 있을 때 뉴럴넷을 이용하여 𝜋의 가치 함수인 𝑣_𝜋(s)를 학습하는 방법
- Q러닝을 큰 문제로 확장하여 뉴럴넷을 이용한 최적의 정책을 찾는 방법
=============================================================================
밸류 네트워크의 학습

뉴럴넷으로 이루어진 가치 함수 𝑣_𝜃 (𝑠)가 있다 - 이 뉴럴넷을 밸류 네트워크라고 부른다.
길이 2인 벡터 인풋 s를 받아서 특정 정책 𝜋를 따랐을 때의 밸류를 리턴하는 뉴럴넷이다.
정확한 표기법 𝑣_𝜃,𝜋 (𝑠)
𝜃는 뉴럴넷의 파라미터, 처음에는 랜덤으로 초기화되어 있다.
목표는 적절한 𝜃를 학습하여 𝑣_𝜃,𝜋 (𝑠)가 각 상태별로 올바른 밸류를 출력하도록 하는 것이다.

뉴럴넷을 학습하려면 예측과 정답 사이의 차이를 뜻하는 손실 함수를 정의해야 한다.
이를 위해서는 정답에 해당하는 값이 필요하다. 하지만 알지 못하므로 가치 함수의 정답지로 사용할 수 있는 몇 가지 방법론이 있다.
기댓값을 사용하는 것이다.
손실 함수를 정의했으면 𝜃에 대한 그라디언트를 계산한다.
체인 룰????
상수 값은 그저 스텝 사이즈이기 때문에 나중에 𝛼를 이용해 조절할 수 있다.

수많은 샘플을 통해 𝑣_𝜃 (𝑠)와 𝑣_𝑡𝑟𝑢𝑒 (𝑠)를 거의 같게한다.

𝜋가 있을 때 밸류 네트워크 𝑣_𝜋(𝑠)을 학습시킬 수 있다.
하지만 실제 상황에서 실제 가치 함수인 𝑣_𝑡𝑟𝑢𝑒(𝑠)가 주어질 일은 절대 없다.
𝑣_𝑡𝑟𝑢𝑒 (𝑠)가 없다면 정답이 주어지지 않는 것이고, 손실 함수도 정의 불가능하고, 그라디언트 계산도 불가능하다.
𝑣_𝑡𝑟𝑢𝑒 (𝑠)가 주어지지 않아도 그를 대신하는 몇 가지 선택지를 알고 있다. ( 몬테카를로 방법을 사용한 리턴과 TD학습 방법을 사용한 TD타깃 )

첫 번째 대안 : 몬테카를로 리턴
정답 자리에 Gt를 사용할 수 있는 이유는 실제 가치 함수의 정의가 곧 Gt의 기댓값이기 때문이다.
테이블 업데이트에 쓰였던 식이다. 뉴럴넷을 업데이트하려면 손실 함수가 정의되야 한다.
𝑣_𝑡𝑟𝑢𝑒 (𝑠) 자리에 Gt를 대입한다.
손실 함수가 정의되고 𝜃를 업데이트하는 방식은 동일
- 단점 : 에피소드가 끝나야만 리턴 계산 가능(실시간 업데이트 불가능), 분산이 크다

두 번째 대안 : TD 타깃
TD 학습 방법은 한 스텝 더 진행해서 추측한 값을 이용하여 현재의 추측치를 업데이트하는 방식
리턴 Gt 대신 TD 타깃 사용
* TD타깃 𝑟_(𝑡+1)+𝛾𝑣_𝜃 (𝑠_(𝑡+1))은 변수가 아니라 상수이다.
𝑣_𝜃(𝑠_(𝑡+1))항이 포함되어 있기 때문에 𝜃에 따라서 값이 달라진다.
하지만 업데이트 시점의 𝜃를 이용해 𝑟_(𝑡+1)+𝛾𝑣_𝜃 (𝑠_(𝑡+1))의 값을 계산하면 이 값은 하나의 숫자이다.
상수이기 때문에 손실 함수를 𝜃에 대해 편미분할 때, 𝑣_𝜃 (𝑠_(𝑡+1))을 미분한 결과 0이 된다.

왜??? 𝑣_𝜃 (𝑠_(𝑡+1))를 상수 취급해야 할까?
- 목적지를 변하지 않게 하기 위함(정답지의 값이 움직이지 않고 가만히 있게 되어 안정적인 학습이 가능해진다.
𝑣_𝜃 (𝑠_(𝑡+1))를 상수 취급하면 내가 목적지를 향해서 가는 것에 해당되고
𝑣_𝜃 (𝑠_(𝑡+1))를 변수 취급하면 목적지를 향해서 가지만 목적지도 나를 향해서 다가오게 된다.
- 타깃이 함께 변하므로 뉴럴넷의 학습을 매우 불안정하게 한다.
- 상수 취급하라의 의미는 𝜃에 대해 편미분할 때 𝑣_𝜃 (𝑠_(𝑡+1))값을 그냥 0으로 만들어버리는 것이다.