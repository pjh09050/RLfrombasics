TD 컨트롤  1 - SARSA

MC컨트롤은 평가 단계에서 MC를 사용한 것이다. " MC 대신 TD를 사용하면 안 될까? "
TD가 MC에 비해 여러 장점이 있다. --> 분산이 훨씬 작다거나, 에피소드가 끝나지 않아도 온라인으로 학습할 수 있다.
MC와 TD는 실제 밸류에 가까워지도록 테이블의 값을 수정해 나가는 과정이다.
TD는 경험으로부터 얻은 샘플을 통해 실제 값을 추측하여 추측치에 가까워지도록 업데이트 하는 방법이다.

정책 이터레이션의 평가 단계에서 TD를 사용할 것이다.
MC 대신 TD

TD를 이용해 q(s,a)를 구해 보자. 그런데 TD를 이용해 Q를 계산하는 접근법을 가리키는 용어가 SARSA이다.
상태 s에서 액션 a를 선택하면 보상 r을 받고 상태 s'에 도착하고, 상태 s'에서 다음 액션 a'을 선택한다. 이 유래로 알고리즘 이름이 SARSA이다.

TD로 V학습 : 𝑉(𝑆)←𝑉(𝑆)+ 𝛼(𝑅+𝛾𝑉(𝑆^′ )−𝑉(𝑆))
TD로 Q학습(SARSA) : Q(𝑆,𝐴)←𝑄(𝑆,𝐴)+ 𝛼(𝑅+𝛾𝑄(𝑆^′,𝐴′)−𝑄(𝑆,𝐴))

TD로 V 학습 : 𝑣_𝜋 (𝑠_𝑡 )=𝔼_𝜋 [𝑟_(𝑡+1)+𝛾𝑣_𝜋 (𝑠_(𝑡+1))]
TD로 Q 학습 : 𝑞_𝜋 (𝑠_𝑡,𝑎_𝑡 )=𝔼_𝜋 [𝑟_(𝑡+1)+𝛾𝑞_𝜋 (𝑠_(𝑡+1),𝑎_(𝑡+1))]

TD타깃에 해당하는 식이 모두 벨만 기대 방정식으로부터 나온 것임을 확인할 수 있다. 기댓값 안의 샘플을 무수히 모으면 결국 실제 가치와 가까워질 것이기 때문에
에이전트를 환경에 던져 놓고, 에이전트가 자신의 정책 함수 𝜋를 이용해 자유롭게 거닐게 하다가 한 스텝의 데이터가 생성될 때마다 이 데이터를 통해 TD 타깃을 계산하여
기존의 테이블에 있던 값들을 조금씩 업데이트해 나가는 방식이다.

SARSA 구현
- 환경이 바뀐 것이 아니라 에이전트의 업데이트 방식이 바뀐 것
update_table 함수가 조금 바뀌었다.
MC에서 update_table 함수가 에이전트가 경험한 history 전체를 인자로 받았지만, 트랜지션을 인풋으로 받습니다.
* 트랜지션은 상태 전이 1번을 뜻한다.
* 상태 s에서 a를 해서 보상 r을 받고 상태 s'에 도달했다면 (s, a, r, s')이 하나의 트랜지션이다.
* TD 학습은 샘플 하나만 생기면 바로 업데이트할 수 있기 때문에 HISTORY 대신 트랜지션을 이용해 업데이트 하는 방식으로 바뀐 것이다.
* 업데이트 식도 SARSA 업데이트 식을 그대로 적용하였다.
