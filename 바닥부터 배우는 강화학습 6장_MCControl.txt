MDP를 모를 때 최고의 정책 찾기

Chpater 5는 주어진 정책을 평가하는 방법론일 뿐 최고의 정책을 찾는 방법은 아니다.
Chapter 6는 최고의 정책을 찾는 방법이다.

조건 1. 테이블에 모든 상태의 가치를 담을 수 있을 정도로 상태의 개수나 액션의 개수가 적은 작은 MDP 세팅이라는 점.
조건 2. MDP에 대한 정보를 전혀 모르고 있는 상황
---> 이런 상황에서 control 방법 중 가장 대표적인 3가지 방법론 ( 몬테카를로 컨트롤, SARSA, Q 러닝 )

####################################################################################################

6.1 몬테카를로 컨트롤

모델-프리에서는 정책 이터레이션을 사용할 수 없다. 왜??
1. 평가 단계에서 반복적 정책 평가를 사용할 수 없다.(벨만 기대 방정식 2단계를 사용할 수 없어서, 그럼 0단계,1단계를 사용할 수는 없나??)
2. 개선 단계에서 그리디 정책을 만들 수 없다.(상태 전이에 대한 정보를 모르기 때문)

해결 방법
1. 평가 자리에 MC

2. V 대
- v(s) 대신에 상태-액션 가치 함수인 q(s,a)을 알면 그리디 액션을 선택할 수 있다.

결론
- MC를 이용하여 q(s,a) 계산, 평가된 q(s,a)를 이용해 새로운 그리디 정책을 만들고, 이 정책에 대해 또 다시 MC를 이용하여 q(s,a)를 계산 ===== 이 과정 반복
- 최적 정책을 찾는 것이 가능
하지만 한 가지 더 추가해야 된다. ----> 탐색
: 강화학습에서 에이전트가 최적의 해를 찾으려면 에이전트는 주어진 MDP 안의 여러 상태를 충분히 탐색해야 한다.
여러 상태를 보고 각 상태에서 이러저러한 액션들을 모두 해봐야 가장 좋은 액션의 시퀀스를 찾아낼 수 있어서
- 에이전트가 다양한 공간을 탐색할 수 있도록 보장해주는 장치가 필요하다(못 가본 상태가 정말 좋은 상태라면 에이전트는 최적의 해를 찾지 못하게 된다.)

과정
모든 q(s,a) 값을 0으로 초기화 해놓고 학습을 시작했을 경우
MC를 이용해 딱 한 번 업데이트한 순간 q(s, a1)의 값이 0.1로 증가한다.
MC는 실제로 실행한 액션의 q밸류만 업데이트 해주기 때문에, 상태 s에서의 다른 액션들의 밸류인 q(s,a2), q(s,a3),... 는 여전히 0의 값을 갖고 있다.
!!!! 여기서 q(s,a1)의 값이 한 번 치고 나가는 순간 큰일이 난다.
개선 단계에서 그리디 정책을 사용하기 때문에 앞으로 평생 상태 s에서는 a1만 선택될 것이다.
다른 액션들이 선택될 기회가 영구히 사라진다. -----> 다른 액션이 더 좋을 수도 있지 않나???? 
그래서 액션을 항상 그리디하게 선택하면 안되며 에이전트의 탐색을 보장해 주도록 때로는 가치가 높지 않은 다른 액션들을 다양하게 선택해 주어야 한다.
--------> 그러면 어떻게 액션을 선택해야 할까????
랜덤으로 하면 바보같은 에이전트가 될 것이다. "탐색의 정도"를 알맞게 맞춰야 한다. 
방법으로는 입실론 그리디 이다.

3. 그리디 대신 입실론 그리디
입실론이라는 작은 확률만큼 랜덤하게 액션을 선택하고, 1-입실론이라는 나머지 확률은 원래처럼 그리디 액션을 선택한다.
입실론을 0.1이라고 한다면, 우리의 정책은 90%의 확률로 q(s,a)값이 가장 높은 액션 a를 선택할 것이고, 나머지 10%의 확률로 여러 액션 중 랜덤하게 하나의 액션을 선택한다.
-------> 더 좋은 방법은 입실론의 값을 처음에는 높게 하다가 점점 줄여주는 것이다.
	(왜???? 처음에는 환경에 대해 아는 것이 없기 때문에 다양한 액션들을 선택하면서 환경에 대한 정보를 충분히 얻어야 하고, 학습이 어느 정도 진행되고 나면
		이미 얻은 정보를 바탕으로 조금 더 최선의 선택을 내리는 데에 집중한다.)-------> 줄여나간다는 의미로 decaying 입실론 greedy라고 부른다.

수렴할 때까지 N번 반복
- 한 에피소드의 경험을 쌓고
- 경험한 데이터로 q(s,a) 테이블의 값을 업데이트하고 (정책 평가)
- 업데이트된 q(s,a) 테이블을 이용하여 입실론 그리디 정책을 만들고 (정책 개선) -------------> 이 과정 반복

########################################################################################################
q_table : q 밸류를 저장하는 변수(에이전트가 액션을 선택할 때 사용)
select_action : 상태 s를 input으로 받아 s에서 알맞은 액션을 입실론 그리디 방식을 통해 선택 -> 이를 위해 내부에 epsilon이라는 값도 들고 있다.
anneal_eps : epsilon 값을 0.9에서 0.1까지 선형적으로 줄어든다.
update_table : 실제로 테이블의 값을 업데이트 해주는 함수
----> 하나의 에피소드에 해당하는 데이터를 받아서 MC 방법으로 테이블의 값을 업데이트한다.
show_table : 학습이 끝난 후에 상태별로 q(s,a)의 값이 가장 큰 액션을 뽑아서 보여주는 함수
에이전트와 환경을 만들고, 하나의 에피소드가 끝날 때까지 history라는 변수에 상태 전이 과정을 모두 저장해 두었다가, 에피소드가 끝난 순간 해당 변수를 이용해 에이전트 내부의
q테이블을 업데이트한다. 그리고 epsilon 값을 조금씩 줄여준다.