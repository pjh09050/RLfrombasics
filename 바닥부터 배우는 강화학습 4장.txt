4. MDP를 알 때의 플래닝(작은 문제이고 MDP를 알 때)

MDP를 안 다는 것 - 보상 함수와 전이 확률 행렬을 알고 있다는 뜻 (어떤 상태 s에서 액션 a를 실행하면 다음 상태가 어떻게 정해지는지, 보상이 어떻게 될지 미리 알고 있다는 뜻)

MDP를 알 때 이를 이용하여 정책을 개선해 나가는 과정을 플래닝이라고 한다.

테이블 기반 방법론 - 모든 상태 s 혹은 상태와 액션의 페어 (s, a)에 대한 테이블을 만들어서 값을 기록해 놓고, 그 값을 조금씩 업데이트하는 방식
(상태나 액션의 개수가 많지 않아 해당하는 테이블을 만들 수 있어서 가능한 방법론)

===================================================================================================================

4-1. 밸류 평가하기 - 반복적 정책 평가
정책 함수 𝝅 : 4방향 랜덤 이 주어졌고 이때 각 상태 s에 대한 가치 함수 v(s)를 구하는 prediction문제

반복적 정책 평가(Iterative policy evaluation) - 테이블 값들을 초기화한 후, 벨만 기대 방정식을 반복적으로 사용하여 테이블에 적어 놓은 값을 조금씩 업데이트해 나가는 방법론
(MDP의 모든 정보를 알 때 사용 가능)

1단계 - 테이블 초기화
2단계 - 한 상태의 값을 업데이트 ( 보상 함수와 전이 확률 행렬을 알고 있기 때문에 벨만 기대 방정식 2단계 수식 이용 )
	왜 이런 식을 사용하는게 더 정확한가? 현재 테이블에 있는 값은 우리가 정한 무의미한 값이다. 다음 상태의 값만 가지고 업데이트를 하면 무의미한 제자리 걸음이다. 
	하지만 다음 상태의 값과 보상이라는 실제 환경이 주는 정확한 시그널을 섞어서 업데이트를 함으로써 점차 실제 값에 가까워진다.
	마지막 상태의 값은 미래가 존재하지 않기 때문에 항상 0이다.
3단계 - 모든 상태에 대해 2단계 과정 적용 ( 마지막 상태를 제외한 15개의 상태를 업데이트 )
4단계 - 2단계, 3단계 과정을 계속해서 반복 ( 몇 번을 반복해는지 k로 표기)

===================================================================================================================

4-2 최고의 정책 찾기 - 정책 이터레이션 (단계마다 서로 다른 정책의 가치를 평가)
𝜋′은 원래 정책 𝜋보다는 나은 정책 : 그리디 정책 ( 먼 미래까지 보지 않고 그저 눈 앞의 이익을 최대화하는 선택을 취하는 방식 ), 당장 다음 칸의 가치가 높은 칸을 선택하기 때문

주어진 랜덤 정책 𝜋에 대해 밸류를 평가하고, 그에 대해 그리디한 정책을 표시해 봤더니 그 정책이 곧 최적 정책과 일치하게 된 상황이다.
-> 랜덤 정책의 가치를 평가했을 뿐인데 더 나은 정책을 알게 된 것이다.

평가와 개선의 반복
정책 𝜋를 임의의 정책으로 초기화해놓고 시작
1단계. 정책 평가(반복적 정책 평가 방법론 사용) - 고정된 𝜋에 대해 각 상태의 밸류를 구한다. 𝜋를 따랐을 때 각 상태의 가치를 평가하는 일이기 때문에 과정을 정책 평가라고 부른다.
2단계. 정책 개선(그리디 정책 생성) - 새로운 정책 𝜋′를 생성한다. 생성 방법으로는 앞서 정책 평가 단계에서 구한 v(s)를 이용해 v(s)에 대한 그리디 정책을 생성하면 된다.
				𝜋′을 생성하면 𝜋보다 좋은 정책이기 때문에 𝜋′ > 𝜋가 성립하므로 정책의 개선이 발생하는 것이다.
𝜋′이 생성되면 𝜋′에 대해 다시 정책 평가를 진행한다. 즉, 정책의 평가와 개선을 반복하는 것이다.
어느 순간 정책도 변하지 않고, 그에 따른 가치도 변하지 않는 단계에 도달하게 된다. 수렴하는 곳이 바로 최적 정책과 최적 가치가 된다.

𝜋_𝑔𝑟𝑒𝑒𝑑𝑦 : 한 스텝만 그리디 정책으로 움직이고 나머지 모든 칸은 원래 정책으로 움직이는 정책
한 스텝을 그리디하게 움직이고 𝜋를 따르는게 처음부터 𝜋를 따르는 것보다 좋다.
-> 결국 모든 상태에서 그리디한 정책 𝜋′이 원래 정책 𝜋보다 좋으므로 정책이 개선된다는 것

정책 평가 단계에서 모든 스텝을 다 평가할 필요가 없다.
최고의 정책을 찾는 것이 목적!! 정책 이터레이션에서는 가치 함수가 오로지 정책을 찾는 데에만 쓰일 뿐
-> 가치 함수를 끝까지 학습하지 않아도 된다
-> 심지어 한 번만 업데이트해도 된다. ( 다르기만 하면 일단 '개선'은 된 것이기 때문 )

===================================================================================================================

4-3 최고의 정책 찾기 - 밸류 이터레이션 (최적 정책이 만들어내는 최적 밸류 하나만 본다.)
각 테이블에서 각 칸의 최적 밸류를 나타내고 최적 밸류 사이의 관계를 알고 싶은 것이므로 벨만 최적 방정식 2단계 이용
모든 상태에 대하여 최적 가치를 계산한 것이다. 최적 밸류를 알면 최적 정책을 알 수가 있다.
최적 밸류가 가장 높은 칸으로 움직이면 최적 정책이다. 최적 밸류에 대한 그리디 정책이다. 
최적 밸류에 대한 그리디 정책은 먼 미래까지 함께 보는 최적 정책이다.

===================================================================================================================

강화학습의 목적은 가장 많은 보상을 얻을 수 있게 하는 정책을 얻는 것이라고도 할 수 있다.
정책 이터레이션 - 정책이 있고 도구로 가치함수 이용
정책을 따로 두지 않고 가치함수만 가지고 내재적인 정책을 가지는 것이 밸류 이터레이션
정책 이터레이션과 밸류 이터레이션의 장단점이 있다.
밸류 이터레이션은 계산적으로 효율적이고 빠르게 수렴할 수 있지만 현실적이지 않아 MDP에 대한 완전한 지식이 필요하다.
정책 이터레이션은 유연하고 MDP에 대한 불완전한 지식을 처리할 수 있지만 계산 비용이 많이 들고 밸류 이터레이션보다 수렴속도가 느릴 수 있다.