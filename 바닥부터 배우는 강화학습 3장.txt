밸류를 구하는데 뼈대가 되는 수식이 벨만 방정식이다.
- 시점 t에서의 밸류와 시점 t+1에서의 밸류 사이의 관계를 다루고 있으며 또 가치 함수와 정책 함수 사이의 관계도 다루고 있다.
- 재귀 함수 : 자기 자신을 호출하는 함수
1. 벨만 기대 방정식
- s의 밸류 = 시그마(s에서 a를 실행할 확률 * s에서 a를 실행하는 것의 밸류)의 총 합
- s에서 a를 실행하는 것의 밸류 = 즉시 얻는 보상 + 감마*시그마(s에서 a를 실행하면 s'에 도착할 확률 * s'의 밸류)

0단계 식 : 현재 상태의 밸류와 다음 상태의 밸류를 기댓값 연산자를 통해 연결해 놓은 식
2단계 식 : 그 기댓값을 계산하는 식

각 상태 s에서 액션을 선택하면 얻는 보상을 모르는 경우에는 실제로 상태 s에서 액션 a를 해보는 수밖에 없다.
MDP에 대한 정보를 모를 때 학습하는 접근법을 모델-프리 접근법이라고 한다.

2. 벨만 최적 방정식
- 벨만 기대 방정식은 모두 정책이 𝝅로 고정되었을 때의 밸류에 관한 함수였다.
- v*(s)와 q*(s,a)는 최적 밸류에 대한 함수이다.

모든 상태 s에 대해, v𝝅1(s) > v𝝅2(s) 이면 𝝅1 > 𝝅2 이다.
MDP 내의 모든 𝝅에 대해 𝝅* > 𝝅 를 만족하는 𝝅*가 반드시 존재한다.

최적의 정책 : 𝝅*
최적의 밸류 : v*(s) = v𝝅*(s) (𝝅*를 따랐을 때의 밸류)
최적의 액션 밸류 : q*(s,a) = q𝝅*(s,a) (𝝅*를 따랐을 때의 액션 밸류)

* 정책 𝝅가 주어져 있고, 𝝅를 평가하고 싶을 때에는 벨만 기대 방정식
* 최적의 밸류를 찾는 일을 할 때에는 벨만 최적 방정식을 사용한다.