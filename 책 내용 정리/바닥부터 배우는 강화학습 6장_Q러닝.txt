TD 컨트롤 2 - Q 러닝
테이블을 만들어서 Q함수를 기록해 두고, 그 값을 업데이트하는 방식으로 학습 진행

SARSA와 Q러닝 차이
off-policy와 on-policy
On-Policy : 타깃 정책과 행동 정책이 같은 경우 ( 직접 경험 ) - 지금까지 배운 내용
Off-Policy : 타깃 정책과 행동 정책이 다른 경우 ( 간접 경험 ) 

타깃 정책이란??? 강화하고자 하는 목표가 되는 정책. 학습의 대상이 되는 정책이고 계속해서 업데이트 됨에 따라 점점 강화되는 정책이다.
행동 정책??? 실제로 환경과 상호 작용하며 경험을 쌓고 있는 정책을 뜻한다.

off-policy에서 타깃 정책은 자신이 강화하고자 하는 것은 자신의 뇌 안에 있는 것, 실제로 환경에서 경험을 쌓고 있는 정책은 민준이의 정책이다.

off-policy와 지도학습의 차이
지도학습 : 행동을 그대로 따라하는 방식 ( 모든 상황에서 최적이라면 지도 학습은 효과적인 방법이다. )
off-policy : 무조건 행동을 따라하지 않는 방식 ( 좋았던 경험은 그대로 따라하기도 하지만, 결과가 좋지않던 경험은 수정하기도 한다. )
	  경험을 쌓는 부분은 온전히 의존하고 있지만 플레이 방식은 다를 수 있는 상황이다.

off-policy의 장점
1. 과거의 경험을 재사용할 수 있다.
- 𝜋0를 업데이트 하기 위해 경험 100개를 쌓았다고 한다면, 𝜋0를 𝜋1으로 아주 조금 업데이트 했다.
- on-policy 방법으로는 다시 𝜋1을 학습하려면 𝜋1을 이용해서 경험을 다시 쌓아야한다. ( 𝜋0와 𝜋1은 다른 정책이기 때문에 ) ( 데이터의 효율성 측면에서 단점 )
- off-policy 방법론의 경우 타깃 정책과 행동 정책이 달라도 되기 때문에 과거의 정책 𝜋0가 경험한 샘플들을 𝜋1의 업데이트에 그대로 재사용할 수 있다. 
	(𝜋1뿐만 아니라 𝜋1000까지 가더라도 가장 처음에 쌓은 데이터를 재사용할 수 있게 되니 효율성 측면에서 좋다.)
2. 사람의 데이터로부터 학습할 수 있다.
- off-policy 방법론의 행동 정책에는 어떤 정책도 가져다 놔도 됩니다. 
- 행동 정책과 타깃 정책 사이의 차이가 얼마나 큰지에 따라 학습의 효율성은 달라지겠지만, 행동 정책은 환경과 상호작용하며 경험을 쌓을 수만 있다면 상관없다.
- (s, a, r, s')로 이루어진 데이터만 있다면 학습에 쓸 수 있다.
3. 일대다, 다대일 학습이 가능하다.
- off-policy 학습을 이용하면 이중에서 단 1개의 정책만 경험을 쌓게 두고, 그로부터 생성된 데이터를 이용해 동시에 여러 개의 정책을 학습시킬 수 있다.
- 반대로, 동시에 여러 개의 정책이 겪은 데이터를 모아서 단 1개의 정책을 업데이트 할 수도 있다.
- 한 에이전트의 정책은 굉장히 실험적인 행동을 하여 환경의 여러 상태들을 방문하며 다양한 경험을 쌓는데 특화되어 있고,(행동정책)
  다른 에이전트의 정책은 현재까지 알고 있는 정보를 바탕으로 최고의 성능을 내는데 집중한다면 이 두 에이전트가 겪는 경험의 질은 전혀 다를 것이다.(타깃정책)

----> 타깃 정책과 행동 정책이 달라도 괜찮고, 같아도 괜찮다. off-policy가 on-policy 상위 호환이라고 볼 수도 있다.

================================================================================================================

Q러닝의 이론적 배경 - 벨만 최적 방정식
q*(s,a)는 이 세상에 존재하는 모든 정책들 중에 얻게되는 가장 좋은 정책을 따를 때의 가치를 나타내는 함수이다.
q*를 알게 되는 순간 우리는 주어진 MDP에서 순간마다 최적의 행동을 선택하며 움직일 수 있다. 상태마다 q*의 값이 가장 높은 액션을 선택하면 되기 때문이다.
우리의 목적은 q*를 찾는 것이다. 이 해답은 벨만 최적 방정식에 있다.
벨만 최적 방정식 2단계는 q*(s,a)가 q*(s',a')과 어떤 관계가 있는지를 재귀적으로 나타내는 식이다. --> MDP에 대한 정보를 전혀 모르기 때문에 위의 식을 이용할 수는 없다.
실제 액션 a를 해보기 전까지는 다음 상태가 어디가 될지, 보상은 얼마나 받을지 알 수 없다.
---> 보통 이런 상황에는 기댓값을 사용한다. 그래서 벨만 최적 방정식 0단계를 이용하겠다. ---> 이 식을 이용하여 TD 학습을 하면 끝
q*(s,a)의 값을 담아 놓기 위한 테이블을 만들고, 괄호안에 있는 식을 정답이라 보고 그 방향을 조금씩 업데이트해 나가도록 하겠다.

SARSA와 Q러닝의 차이
Q러닝 TD타깃부분은 벨만 최적 방정식의 기댓값 안의 항이다.
SARSA : 행동 정책과 타깃 정책이 같다.
Q러닝 : 행동 정책과 타깃 정책이 다르다.
행동 정책에는 탐험을 위한 입실론 값이 들어가 있는 반면 타깃 정책은 순수하게 가장 Q값이 높은 액션을 선택하는 방식인 그리디 정책을 따르고 있다.

왜 Q러닝은 off-policy 학습이 가능한가??? (SARSA와 Q러닝의 업데이트 식의 출발점을 생각해봐라)
- SARSA는 벨만 기대 방정식, Q러닝은 벨만 최적 방정식에 기원을 두고 있다.
- 차이를 보면 SARSA는 𝔼_𝜋이고 Q러닝은 𝔼_𝑠′이다. 
- 𝔼_𝜋는 정책 함수 𝜋를 따라가는 경로에 대해서 기댓값을 취하라는 의미( 𝜋(als)와 확률적인 요소 P가 고려되어 샘플을 하게 됨 )
- 𝔼_𝑠′은 정책 함수 𝜋와 아무런 관련이 없는 항이다. 식을 보아도 𝜋와 관련된 항이 없다. 𝔼_𝑠′를 계산할 떄에 어떠한 정책을 사용해도 상관 없다는 뜻이다.
- 그러니 임의의 정책을 사용하여 주어진 환경 안에서 데이터를 모으면, 그 데이터 안에 P가 모두 반영되어 샘플이 모이고, 샘플을 이용하여 기댓값을 계산할 수 있다.

왜 Q러닝 수식에는 𝜋와 관련된 항이 사라졌을까요????? (벨만 최적 방정식이 사용되었기 때문에)
최적 정책은 환경에 의존적이다. 환경이 정해지면 그에 따라 최적 정책도 정해진다. 환경을 충분히 잘 탐험한다면 최적 정책을 찾을 것이고 탐험하는데 사용되는 정책은 어떤 것이든 상관없다.

