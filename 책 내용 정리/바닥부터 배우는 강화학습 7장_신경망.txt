7.2 인공 신경망의 도입

인공 신경망
신경망의 본질은 유연한 함수이다. 유연성이 너무 뛰어나서 세상의 어떤 복잡한 관계에도 피팅할 수 있을 정도이다.
함수에 포함된 프리 파라미터(free parameter)의 개수를 통해 함수의 유연성을 표현할 수 있다.
1차 함수의 경우 프리 파라미터가 2개, 16차 함수인 경우 17개이다. 신경망은 프리 파라미터가 100만 개를 넘어간다.
----> 유연한 함수에다가 상태별 가치 값 들을 담는다는 것이 Deep RL 학습의 요체이다.

신경망의 개요
길이 3인 벡터를 input으로 받아 값 하나를 리턴하는 함수이다. y = f(x1,x2,x3)
그 안에는 히든 레이어(hidden layer)가 두 층이 쌓여 있다. 각각의 히든 레이어에는  여러 개의 노드(node)로 구성되어 있다.
노드가 옆으로 나란히 이어져서 레이어를 하나 구성하고, 레이어가 층층이 쌓여서 신경망을 구성한다.
신경망은 히든 레이어로 구성되어 있고, 히든 레이어는 노드로 구성되었으니 결국 노드가 신경망의 기본 구성 단위인 것이다.
** n번째 레이어들의 노드들은 모두 n-1번째 레이어의 노드들의 결합으로 이루어진 피쳐이기 때문에
   위층으로 갈수록 더 추상화된 피쳐들이 학습된다.

노드
해당 노드로 들어오는 값들을 선형 결합(linear combination)한 후에 비선형 함수(non-linear activation)를 적용한다.
(x1,x2,x3)을 선형 결합하여 w1x1+w2x2+w3x2+b의 값을 만든 후 g(x)라는 비선형수를 통과시킨다. g(w1x1+w2x2+w3x2+b)

뉴럴넷에서 사용하는 비선형 함수에는 여러 선택지가 있다.(RELU말고 다른 것 찾아보기)
RELU(rectified linear unit)는 g(x)=max(0,x) 형태의 비선형 함수이다.

선형결합
선형결합은 새로운 피쳐(feature)를 만드는 과정이다.
새로만들어진 피쳐는 input 벡터의 피쳐보다 한층 더 추상화(abstract)된 피쳐라고 할 수 있다.
이처럼 학습에 필요한 피쳐가 있다면 스스로 신경망의 파라미터들이 알맞은 값으로 학습된다.

비선형 함수
비선형 함수는 input과 output의 관계가 비선형 관계일 수 있기 때문에 필요한 함수이다.
비선형 함수가 없다면 선형관계만 학습할 수 있을 텐데, 뉴럴넷의 표현력이 실제 자연의 많은 문제를 품기 어려울 것이다.

** 뉴렐넷을 학습한다는 것은 뉴렐넷을 구성하는 파라미터들인 w와 b의 값을 찾는 과정(화살표마다 대응되는 w값, 노드마다 대응하는 b값)

============================================================================================

신경망의 학습 - 그라디언트 디센트
신경망을 어떻게 학습시킬 것인가? 주어진 값의 쌍을 신경망에 어떻게 새겨 넣을 것인가??

함수가 랜덤하게 초기화 되었다는 것은 뉴렐넷의 파라미터 w와 b가 랜덤한 값으로 설정되었다는 뜻이다.

L(w)는 뉴럴넷의 output이 주어진 데이터로부터 틀린 정도를 나타내기 때문에 손실 함수(loss function)이라고 부른다.
우리의 목표는 손실 함수의 값이 줄어들도록 w를 수정하는 것이다.
w를 어떻게 수정해야 할까요???? L(w)의 값을 계산하는데 있어 w가 미치는 영향력을 알아야한다.
L(w) = {1-fw(3)}^2
w를 0.1만큼 증가시키면 L(w)의 값이 0.1만큼 증가하고
w를 0.1만큼 감소시키면 L(w)의 값이 0.7만큼 감소 것이다.
----> 이 영향력을 가리키는 수학적 표현이 미분이다. (L(w)를 w로 미분해보면 그것이 w가 L(w)에 미치는 영향력이다.)
----> 미분 값이 클수록 해당 파라미터의 영향력이 크다고 할 수 있다. (효율적으로 L(w)의 값을 바꾸자)

하지만 뉴렐넷에는 여러 개의 w가 들어있다. (파라미터 각각에 대해 미분하는 것을 --- 편미분이라고 한다.)
f를 w1부터 w100까지 각각의 파라미터에 대해 편미분하여 벡터를 만든 것 --- 그라디언트(gradient)라고 한다.
그라디언트를 구하면 w를 그라디언트 방향으로 아주 조금 이동시켜주면 된다.
아주 조금씩 이동시키는 이유는 방향을 너무 많이 움직여 버리면 구한 그라디언트가 모두 바뀌기 때문이다.
얼만큼 이동시킬지는 𝛼라는 상수를 통해 정해진다.
𝛼는 업데이트 크기를 결정하는 상수로, 러닝 레이트(learning rate) 혹은 스텝 사이즈(step size)라고 부른다. (0.01같은 작은 수로 생각)
그라디언트에 𝛼라는 상수를 곱하여 원래 값에서 빼준다. (그 이유는? 목적 함수를 최소화하고 싶기 때문이다.)(최대화는 그라디언트를 더해준다.)
-----> 그라디언트를 계산하여 파라미터를 업데이트하는 방식으로 목적함수를 최소해 나가는 과정을 그라디언트 디센트라고 한다.

w를 w'으로 업데이트 하고 나면 L(w')의 값은 L(w)보다 분명 줄어들었을 것이다.
이 과정을 여러 번 업데이트를 하는 것으로 함수가 데이터를 기억하게 되는 과정이다.

============================================================================================

파이토치를 이용한 신경망의 학습 구현
텐서플로우나 파이토치 같은 다양한 자동 미분 라이브러리를 사용한다. (역전파 알고리즘 사용)
역전파 알고리즘은 파라미터가 100만 개일 때 100만 번 편미분하는 것을 방지하고자 뒤에서부터 중간중간 미분 값들을 캐싱했다가
재사용하며 효율적으로 그라디언트를 계산하는 방법론

함수가 만들어내는 데이터만 관찰할 수 있고, 실제 세계에서 데이터를 만들어 내는 함수를 알수 없고, 근사하는 것이 목적이다.
RELU라는 활성 함수 사용
