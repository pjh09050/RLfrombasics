2.1 마르코프 프로세스(Markov Process)
- 미리 정의된 어떤 확률 분포를 따라서 상태와 상태 사이를 이동해 다니는 여정.
- 현재 상태에서 다음 상태로 넘어가는 상태전이라고 한다. 하나의 상태에서 뻗어나가는 화살표의 합은 항상 100%이다.
- 마르코프 프로세스의 첫 상태는 s0입니다. 모든 여정은 s0에서 시작한다.
- 상태의 집합 S : 가능한 상태들을 모두 모아놓은 집합
- 전이 확률 행렬 P : 상태 s에서 다음 상태 s'에 도착할 확률(예 : s0에서 s2에 도달할 확률 ps0s2=0.6)
- 시점 t에서 상태가 s였다면 t+1에서의 상태가 s'이 될 확률

2.2 마르코프 리워드 프로세스(MP + Reward)
- R(보상)과 감마(감쇠 인자) 추가
- 보상 함수 R : 어떤 상태 S에 도착했을 때 받게 되는 보상
- E(기댓값) : 특정 상태에 도달했을 때 받는 보상이 매번 조금씩 다를 수도 있기 때문
- 감마 : 0~1사이의 숫자, 미래 얻을 보상에 비해 당장 얻는 보상을 얼마나 더 중요하게 여길 것인지를 나타내는 파라미터
- 리턴(감쇠된 보상의 합) : 현재부터 미래에 얻게 될 보상의 합
- 에피소드 : 하나의 여정
- 강화학습은 보상의 합인 리턴을 최대화하는 것

- 감마가 필요한 이유
1. 수학적 편리성
- 감마를 1보다 작게 해줌으로써 리턴Gt가 무한의 값을 가지는 것을 방지
2. 사람의 선호 반영
- 기본적으로 당장 벌어지는 보상을 더 선호한다.
3. 미래에 대한 불확실성 반영

MRP에서 각 상태의 밸류 평가하기
- 어떤 상태를 평가할 때에는 그 시점으로부터 미래에 일어날 보상을 기준으로 평가해야 한다.
- 리턴은 그 시점부터 이후에 받을 보상들을 (감쇠하여) 더한 값이다.
- 같다고는 볼 수 없다. 왜냐하면 리턴이라는 값이 매번 바뀌기 때문이다. 그렇기 때문에 리턴의 기댓값을 사용하는 것이다.
에피소드 샘플링
- 매번 에피소드가 어떻게 샘플링되는냐에 따라 리턴이 달라진다.
- 샘플링을 통해서 어떤 값을 유추하는 방법론을 사용하게된다.(Monte-Carlo 접근법)
상태 가치 함수(State Value Function)
- 상태를 인풋으로 넣으면 그 상태의 밸류를 아웃풋으로 출력하는 함수
- 어떤 상태 s의 밸류 v(s)는 기댓값을 이용하여 정의

* MP와 MRP만 가지고는 순차적 의사결정 문제를 할 수 없다.(의사결정이 없기 때문)
* 자신의 의사를 가지고 행동하는 주체인 에이전트가 필요하다.

2.3 마르코프 결정 프로세스(MRP + Agent)
- 에이전트는 각 상황마다 액션(행동)을 취한다.
- 해당 액션에 의해 상태가 변하고 그에 따른 보상을 받습니다.
- A가 추가되면서 MRP에서의 정의와 약간씩 다름
- 액션의 집합 A : 에이전트가 취할 수 있는 액션들을 모아놓은 것, 에이전트는 스텝마다 액션의 집합 중에서 하나를 선택하여 액션을 취하며, 그에 따라 다음에 도착하게 될 상태가 달라진다.
- 각 상태 s에 따라 어떤 액션 a를 선택해야 보상의 합을 최대로 할 수 있는가

정책 또는 정책 함수 : 각 상태에서 어떤 액션을 선택할지 정해주는 함수
- 상태 s에서 액션 a를 선택할 확률
- 각 상태에서 할 수 있는 모든 액션의 확률 값을 더하면 1이 되어야 한다.
- 정책 함수는 에이전트 안에 존재하는 점이다.
상태가치함수 : s부터 끝까지 정책을 따라서 움직일 떄 얻는 리턴의 기댓값
액션가치함수(상태-액션 가치 함수) : q(s,a), 상태에 따라 액션이 달라지기 때문에 액션만 따로 떼어내서 평가할 수 없다.

2.4 Prediction과 Control
MDP가 주어졌을 때, 태스크는 Prediction과 Control이 있다.
- Prediction : 𝜋가 주어졌을 때 각 상태의 밸류를 평가하는 문제, 해당 상태의 밸류를 예측하는 것이 목적
- Control : 최적 정책 𝜋*를 찾는 문제, 이 세상에 존재하는 모든 𝜋중에서 가장 기대 리턴이 큰 𝜋를 뜻한다.
- 𝜋*를 따를 때의 가치 함수를 최적 가치 함수 v*라고 표기한다. 
- 𝜋*, v*를 찾았다면 MDP는 풀렸다고 말할 수 있다.