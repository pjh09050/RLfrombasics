1dsarsa는 on-policy, sarsa는 현재 정책을 기반으로 학습, 
sarsa는 현재 상태에서 가능한 모든 행동에 대해 그 다음 상태의 보상과 다음 행동을 평가하고, 이를 토대로 다음 행동을 선택
sarsa의 학습 과정에서 사용되는 정책은 에이전트의 행동 선택에 직접적인 영향을 미치므로, sarsa는 on-policy 알고리즘이다.

q-learning은 off-policy, q-learning은 최적의 정책을 학습하는 것을 목표
q-learning은 에이전트가 현재 상태에서 가능한 모든 행동 중에서 가장 높은 보상을 얻을 수 있는 행동을 선택하고, 이에 대한 Q값 업데이트
q-learning은 현재의 행동이 아닌, 에이전트가 추구하는 최적 정책에 대한 행동-가치 함수를 학습하므로, off-policy 알고리즘이다.

sarsa는 at+1을 행동 정책으로 얻는다. 과거의 판단(행동정책으로부터 얻어낸 action)으로 얻어낸 Q값
q-learning은 at+1을 타겟 정책으로 얻는다. 현재의 재판단(update된 q를 maximize하는 action을 뽑기 때문에 좀 더 나은 Q값)으로 얻어낸 Q값

sarsa는 과거 허접한 Q로 at+1을 이미 뽑아둔 상태이고
q-learning은 현재까지 update된 최신 Q로 at+1을 뽑는다.

sarsa는 현재 정책에 따라 학습하기 때문에 안정적으로 수렴
q-learning은 최적 정책을 추구하기 때문에 수렴이 더 어렵지만,
최적 정책과 다른 정책을 사용하여 학습할 수 있기 때문에 더 많은 상황에 적용 가능

sarsa는 에이전트가 행동을 선택할 때 미래의 행동도 고려해야 하는 경우에 적합
q-learning은 최적 정책을 학습하는 것이 목표이기 때문에, 에이전트가 항상 최적의 행동을 선택해야 하는 경우에 적합

1. 몬테카를로 컨트롤
- 에피소드가 끝난 후, 해당 에피소드에서 방문한 상태-행동 쌍에 대한 보상을 평균내어 q값을 업데이트
- 에피소드가 끝날 때까지 기다려야 하기 때문에, 시간이 많이 소요됨

2. sarsa
- 현재 상태에서 선택한 행동과 다음 상태에서 선택한 행동에 대한 보상을 이용하여 q값 업데이트
- on-policy 이기 때문에, 현재 정책에 따라 행동을 선택하며, 학습 시에도 해당 정책을 따른다.
- 벨만 기대 방정식

3. q-learning
- q-learning은 다음 상태에서 최대 q값을 이용하여, q값 업데이트
- 벨만 최적 방정식
===============================================================================================
MC vs SARSA vs Q러닝 

1. 업데이트 방식
- MC : 에피소드가 끝난 후, 방문한 상태-행동 쌍에 대한 보상을 평균내어 업데이트
- SARSA : 현재 상태에서 선택한 행동과 다음 상태에서 선택한 행동에 대한 보상을 이용하여 업데이트
- Q러닝 : 다음 상태에서 최대 Q값을 이용하여 업데이트

2. On-policy vs Off-policy
- MC, SARSA는 On-policy이고, 현재 정책에 따라 행동을 선택하고, 학습 시에도 해당 정책을 따른다.
- Q러닝은 off-policy이고, 현재 정책과 다른 정책을 따르면서 학습할 수 있다.

3. 미래 행동 고려 여부
- MC와 SARSA는 미래의 행동을 고려한다.
- Q러닝은 미래의 행동을 고려하지 않는다.

4. 수렴 속도
- MC는 에피소드가 끝날 때까지 기다려야 하기때문에 시간이 많이 소요될 수 있다.
- SARSA는 에피소드가 끝나지 않아도 Q값을 업데이트 할 수 있기 때문에, 수렴이 느릴 수 있다.
- Q러닝은 학습이 빠르고, 수렴이 안정적이다.