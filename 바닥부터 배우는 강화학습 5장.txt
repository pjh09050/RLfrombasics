5. MDP를 모를 때 밸류 평가하기

MDP를 모른 다는 것 - 보상 함수와 전이 확률을 모를 때 ( 실제로 액션을 해 보기 전까지는 보상을 얼마나 받을지도 모르고, 어떤 상태로 이동하게 될 지 확률 분포도 전혀 모르는 상황 )
-> 모델 프리 라고 부른다.

모델 - 강화 학습에서 환경의 모델 ( 에이전트의 액션에 대해 환경이 어떻게 응답할지 예측하기 위해 사용하는 모든 것 )

모델 프리 상황에서의 prediction, 즉 𝜋가 주어졌을 때 가치를 평가하는 2가지 방법
- 몬테카를로 ( Monte Carlo Method )
- TD ( Temporal difference )

===================================================================================================================
5-1 몬테카를로 학습
몬테카를로 - 무언가 측정하기 어려운 통계량이 있을 때 여러 번 샘플링하여 그 값을 가늠하는 기법 ( 대수의 법칙에 의해 각 상태의 밸류 예측치는 점점 정확해진다. )

몬테카를로 학습 알고리즘
정책 : 4방향 랜덤

1단계 - 테이블 초기화 ( N(s)와 V(s) 필요 , N(s) : s를 총 몇 번 방문했는지 , V(s)는 해당 상태에서 경험했던 리턴의 총합을 기록 )
2단계 - 경험 쌓기
3단계 - 테이블 업데이트
4단계 - 밸류 계산 ( 2단계, 3단계 과정을 충분히 반복, 최종적으로 리턴의 평균을 구한다. )

조금씩 업데이트하는 버전 ( 에피소드가 1개 끝날 때마다 테이블의 값을 조금씩 업데이트 , N(s)의 값을 사용안해도됨)
- 𝛼가 얼만큼 업데이트할지 그 크기를 결정해주는 파라미터 ( 𝛼가 클수록 한 번에 크게 업데이트, 작을수록 조금씩 업데이트 )

G(t)가 V(st)보다 크면 𝛼에 곱해진 값이 양수가 되어서 기존의 V(st) 값을 더 크게 만들라는 뜻
G(t)가 V(st)보다 작으면 𝛼에 곱해진 값이 음수가 되어서 기존의 V(st) 값을 더 작게 만들라는 뜻

몬테카를로 학습 구현
환경 - 에이전트의 액션을 받아 상태변이를 일으키고, 보상을 줌
에이전트 - 4방향 랜덤 정책을 이용해 움직임
경험 쌓는 부분 - 에이전트가 환경과 상호작용하며 데이터를 축적
학습하는 부분 : 쌓인 경험을 통해 테이블을 업데이트

===================================================================================================================

5-2 Temporal Difference 학습

MC 의 단점 : 업데이트를 하려면 에피소드가 끝날 때까지 기다려야 한다. 업데이트를 위해서는 리턴이 필요한데 리턴은 에피소드가 끝나기 전까지는 알 수 없기 때문에 MC는 제한적이다.
-> 반드시 종료하는 MDP(terminating MDP)에서만 사용 가능

에피소드가 끝나기 전에 밸류 값을 업데이트하고 싶다면 종료하지 않는 MDP(non-terminating MDP)
리턴이 존재하지 않는데 어떤 값을 이용해 업데이트 해야 할까???? 밸류라는 것이 결국 리턴의 기댓값이다.
-> '추측을 추측으로 업데이트하자' 가 TD의 재미있는 접근 방법이다.

MC : "G(t)는 𝑣_𝜋 (𝑠)의 불편 추정량이다. 편향되지 않는 뜻
TD : 벨만 기대 방정식 0단계 수식, 𝑹(𝒕+𝟏)+𝜸𝒗𝝅(𝒔(𝒕+𝟏))의 기댓값이 𝑣_𝜋 (𝑠)이기 때문이다. 𝑹(𝒕+𝟏)+𝜸𝒗𝝅(𝒔(𝒕+𝟏))을 TD 타깃이라고 부른다.
-> 값을 한 개씩 얻을 때마다 테이블에 원래 쓰여 있던 값을 𝑹(𝒕+𝟏)+𝜸𝒗𝝅(𝒔(𝒕+𝟏)) 방향으로 조금씩 업데이트 할 수 있다.

Temporal Difference 학습 알고리즘 ( MC와 다른점 : 업데이트 수식과 업데이트 시점 )
- 업데이트 수식 변화

===================================================================================================================

5-3 몬테카를로 VS TD
- 학습 시점 : TD 
- 편향성 : MC (TD는 다음 상태의 정확한 밸류를 모르기 때문에), 실제 TD타깃은 불편 추정량이지만 우리가 사용하는 TD타깃은 대문자 V로 편향된 값이다.
	-> TD 타깃은 샘플을 무한히 모아서 지속적으로 업데이트해도 실제 가치에 다가가는 보장은 없다.
	-> 쓰레기 아닌가???? 테이블 룩업과 하나의 조건(TD-zero)이 추가로 붙으면 TD 타깃이 불편 추정량이라는 정리도 있고, 
		   	후에 문제가 커질 때에 뉴럴넷 등의 합수를 도입하게 되면 불편추정량이라는 보장은 없지만 그럼에도 불구하고 실제로 동작을 잘 한다.

- 분산 : TD는 한 샘플만 보면 업데이트가 가능하기 때문에 분산이 작다. MC는 확률적 결과로 이루어진다.
샘플의 평균이 정답이지, 샘플 자체는 정답과 다른 값을 의미, 분산이 클수록 학습을 힘들게 한다.
TD는 MC에 비해 분산을 많이 줄여준다.

===================================================================================================================

5-4 MC와 TD의 중간?
MC는 편향성이 없다는 장점이 있고 TD는 변동성이 적다는 장점이 있다. 

n 스텝 TD
TD 타깃을 생각해보면, 한 스텝만큼 진행하여 실제 보상을 관찰하고, 도착한 상태 st+1의 가치를 가치함수 V를 이용한 추측을 통해 정의하였다.
꼭 한 스텝이 아니라, 두 스텝 여러 스텝도 가능하다. 이와 같은 방법을 3-스텝, 4-스텝, n-스텝이라고 한다.

n=무한대 : 식이 있다.

n-step 리턴에서 n이 무한으로 가면 리턴 값이 된다.
MC는 TD의 한 사례인 셈이다. 한 극단에서는 MC가, 다른 한 극단에는 TD-zero(N=1인 경우 TD 타깃을 이용하여 업데이트하는 방법론을 TD(0)라고 표기하며 TD-zero라고 읽는다.)가 있다.

n이 커질수록 점점 TD-zero에서 MC에 가까워지며 그와 동시에 편향성과 분산에 관한 성질도 바뀌게 된다.
n은 몇으로 하는게 좋은가?? 정답은 없지만 이와 같은 개념들이 도움이 될 수 있다.